{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b91d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake News Classification Pipeline\n",
    "# =================================\n",
    "# Este script implementa un pipeline de clasificación de titulares de noticias\n",
    "# para distinguir entre noticias reales (1) y falsas (0).\n",
    "# Incluye preprocesamiento, vectorización con TF-IDF y embeddings (si está disponible), comparativa de modelos,\n",
    "# y opción de análisis de sentimiento. Además, se proporciona un boceto básico para una app en Streamlit.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ff9a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Preprocesamiento ----------\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1210344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Limpia y normaliza texto: minusculas, elimina HTML, URLs y puntuación,\n",
    "    tokeniza, elimina stopwords y lematiza.\n",
    "    \"\"\"\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords = set(stopwords.words(language))\n",
    "        self.lemma = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'<.*?>', ' ', text)                     # quita HTML\n",
    "        text = re.sub(r'http\\S+|www\\S+', ' ', text)          # quita URLs\n",
    "        text = re.sub(r'[^a-z\\s]', ' ', text)                 # quita puntuación y números\n",
    "        tokens = text.split()\n",
    "        tokens = [self.lemma.lemmatize(tok) for tok in tokens if tok not in self.stopwords]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.apply(self.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06419b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Carga de datos ----------\n",
    "train = pd.read_csv(\"training_data.csv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"testing_data.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a08b9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataset de entrenamiento: Index(['label', 'title'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train.columns = ['label', 'title']\n",
    "test.columns = ['label', 'title']\n",
    "\n",
    "train['label'] = train['label'].map({'FAKE': 0, 'REAL': 1})\n",
    "test['label'] = test['label'].map({'FAKE': 0, 'REAL': 1})\n",
    "\n",
    "print(\"Columnas del dataset de entrenamiento:\", train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e270d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumimos columnas en train: 'title', 'label'; test: 'title' y marcador en 'label' = 2\n",
    "X = train['title']\n",
    "y = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c58038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar y corregir la columna 'label' en el DataFrame de entrenamiento\n",
    "if train['label'].isna().all():\n",
    "    # Intentar mapear los valores de la columna 'label' en base a los datos originales\n",
    "    train['label'] = train['title'].apply(lambda x: 0 if 'FAKE' in x.upper() else (1 if 'REAL' in x.upper() else np.nan))\n",
    "\n",
    "# Actualizar X y y después de corregir la columna 'label'\n",
    "X = train['title']\n",
    "y = train['label']\n",
    "\n",
    "# Eliminar filas con valores NaN en y\n",
    "X_clean = X[~y.isna()]\n",
    "y_clean = y[~y.isna()]\n",
    "\n",
    "# Verificar que X_clean y y_clean no estén vacíos\n",
    "if X_clean.empty or y_clean.empty:\n",
    "    raise ValueError(\"Los datos de entrada están vacíos después de eliminar valores NaN. Verifique los datos de entrada.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f11620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5385be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Definición de pipelines ----------\n",
    "\n",
    "# Pipeline con TF-IDF + Logistic Regression\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('preprocess', TextPreprocessor()),\n",
    "    ('tfidf', TfidfVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb052ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f0a807e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184f95dcdff249918e90c32d176bdab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alber\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a6209283e94c0e8fa8961de0fb1323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270bf6266fe9408685cd28a1607ff9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562c7c941512433a971fde9df2998647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47cb5163fb84e7b87e9f10beefa6130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc23f5007f27491cab9432b1283b5e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915b3d438da74c13a2ec7dfdef9bf545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31192ce642884278874c9650b9043fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc322af4b476412b89ba3a0596c0f687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3b4ad67a294b65b2e0870412d87e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879dafc8949e484fa6fce4cccb53ecf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelo = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e56224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline con embeddings preentrenados + Random Forest\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "class EmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        return np.array(self.model.encode(X.tolist(), show_progress_bar=True))\n",
    "\n",
    "embedding_pipeline = Pipeline([\n",
    "    ('preprocess', TextPreprocessor()),\n",
    "    ('embed', EmbeddingTransformer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3e78116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluando TF-IDF + LogisticRegression ---\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.94      0.97        52\n",
      "         1.0       0.96      1.00      0.98        77\n",
      "\n",
      "    accuracy                           0.98       129\n",
      "   macro avg       0.98      0.97      0.98       129\n",
      "weighted avg       0.98      0.98      0.98       129\n",
      "\n",
      "Confusion Matrix:\n",
      " [[49  3]\n",
      " [ 0 77]]\n",
      "ROC AUC: 0.997\n",
      "\n",
      "--- Evaluando Embeddings + RandomForest ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00a18e0a99e4504bcc1ba633c751641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ce68abeb944bf7b1c4174b95d5c9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b417c48722b64feebebb79389c51a290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.94      0.96        52\n",
      "         1.0       0.96      0.99      0.97        77\n",
      "\n",
      "    accuracy                           0.97       129\n",
      "   macro avg       0.97      0.96      0.97       129\n",
      "weighted avg       0.97      0.97      0.97       129\n",
      "\n",
      "Confusion Matrix:\n",
      " [[49  3]\n",
      " [ 1 76]]\n",
      "ROC AUC: 0.997\n"
     ]
    }
   ],
   "source": [
    "# ---------- Entrenamiento y evaluación ----------\n",
    "\n",
    "def evaluate_model(pipeline, X_tr, X_te, y_tr, y_te):\n",
    "    pipeline.fit(X_tr, y_tr)\n",
    "    preds = pipeline.predict(X_te)\n",
    "    proba = pipeline.predict_proba(X_te)[:,1] if hasattr(pipeline, 'predict_proba') else None\n",
    "    print(\"Classification Report:\\n\", classification_report(y_te, preds))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_te, preds))\n",
    "    if proba is not None:\n",
    "        print(f\"ROC AUC: {roc_auc_score(y_te, proba):.3f}\")\n",
    "\n",
    "print(\"\\n--- Evaluando TF-IDF + LogisticRegression ---\")\n",
    "evaluate_model(tfidf_pipeline, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"\\n--- Evaluando Embeddings + RandomForest ---\")\n",
    "evaluate_model(embedding_pipeline, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "707ca6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'neg': 0.25, 'neu': 0.75, 'pos': 0.0, 'compound': -0.4601}, {'neg': 0.0, 'neu': 0.792, 'pos': 0.208, 'compound': 0.2732}, {'neg': 0.258, 'neu': 0.517, 'pos': 0.225, 'compound': -0.2244}, {'neg': 0.287, 'neu': 0.612, 'pos': 0.101, 'compound': -0.6444}, {'neg': 0.154, 'neu': 0.846, 'pos': 0.0, 'compound': -0.2942}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ---------- Análisis de Sentimientos (opcional) ----------\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def sentiment_scores(texts):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return [sia.polarity_scores(t) for t in texts]\n",
    "\n",
    "# Ejemplo:\n",
    "print(sentiment_scores(X_test[:5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
